version: '3.8'

services:
  ai-task-processor:
    build: .
    container_name: ai-task-processor
    network_mode: host
    environment:
      - API_BASE_URL=${API_BASE_URL:-http://host.docker.internal:3000}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ORY_PROJECT_SLUG=${ORY_PROJECT_SLUG}
      - OAUTH2_CLIENT_ID=${OAUTH2_CLIENT_ID}
      - OAUTH2_CLIENT_SECRET=${OAUTH2_CLIENT_SECRET}
      - OAUTH2_SCOPE=${OAUTH2_SCOPE:-read write}
      - POLLING_INTERVAL_SECONDS=${POLLING_INTERVAL_SECONDS:-30}
      - CONCURRENCY_LIMIT=${CONCURRENCY_LIMIT:-5}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - METRICS_PORT=8001
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-30}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-60}
      - RETRY_BACKOFF_FACTOR=${RETRY_BACKOFF_FACTOR:-2.0}
      - CIRCUIT_BREAKER_THRESHOLD=${CIRCUIT_BREAKER_THRESHOLD:-5}
      # Ollama configuration
      - PROCESSING_MODE=${PROCESSING_MODE:-openai}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-120}
      - OLLAMA_MAX_RETRIES=${OLLAMA_MAX_RETRIES:-3}
      - OLLAMA_MODEL_DOWNLOAD_TIMEOUT=${OLLAMA_MODEL_DOWNLOAD_TIMEOUT:-600}
      # Rate Limiting configuration
      - RATE_LIMIT_ENABLED=${RATE_LIMIT_ENABLED:-true}
      - RATE_LIMIT_STRATEGY=${RATE_LIMIT_STRATEGY:-rolling}
      - RATE_LIMIT_STORAGE_PATH=${RATE_LIMIT_STORAGE_PATH:-/app/data/rate_limits.db}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-0}
      - RATE_LIMIT_PER_HOUR=${RATE_LIMIT_PER_HOUR:-0}
      - RATE_LIMIT_PER_DAY=${RATE_LIMIT_PER_DAY:-0}
      - RATE_LIMIT_PER_WEEK=${RATE_LIMIT_PER_WEEK:-0}
      - RATE_LIMIT_PER_MONTH=${RATE_LIMIT_PER_MONTH:-0}
    ports:
      - "8001:8001"
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Ollama service for local LLM processing
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped

  # Optional: Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

secrets:
  oauth2_client_secret:
    file: ./secrets/oauth2_client_secret.txt
  openai_api_key:
    file: ./secrets/openai_api_key.txt

networks:
  ai-network:
    driver: bridge
    name: ai-task-network

volumes:
  prometheus_data:
  grafana_data:
  ollama_models: