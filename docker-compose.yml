version: '3.8'

services:
  ai-task-processor:
    build: .
    container_name: ai-task-processor
    entrypoint: >
      sh -c "python run.py"
    environment:
      - API_BASE_URL=${API_BASE_URL:-http://host.docker.internal:3000}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ORY_PROJECT_SLUG=${ORY_PROJECT_SLUG}
      - OAUTH2_CLIENT_ID=${OAUTH2_CLIENT_ID}
      - OAUTH2_CLIENT_SECRET=${OAUTH2_CLIENT_SECRET}
      - OAUTH2_SCOPE=${OAUTH2_SCOPE:-read write}
      - POLLING_INTERVAL_SECONDS=${POLLING_INTERVAL_SECONDS:-30}
      - CONCURRENCY_LIMIT=${CONCURRENCY_LIMIT:-5}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - METRICS_PORT=8001
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-30}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-60}
      - RETRY_BACKOFF_FACTOR=${RETRY_BACKOFF_FACTOR:-2.0}
      - CIRCUIT_BREAKER_THRESHOLD=${CIRCUIT_BREAKER_THRESHOLD:-5}
      # Ollama configuration
      - PROCESSING_MODE=${PROCESSING_MODE:-openai}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-120}
      - OLLAMA_MAX_RETRIES=${OLLAMA_MAX_RETRIES:-3}
      - OLLAMA_MODEL_DOWNLOAD_TIMEOUT=${OLLAMA_MODEL_DOWNLOAD_TIMEOUT:-600}
      - SUPPORTED_MODELS=${SUPPORTED_MODELS:-["nomic-embed-text","dengcao/Qwen3-Embedding-0.6B:Q8_0"]}
      # Rate Limiting configuration
      - RATE_LIMIT_ENABLED=${RATE_LIMIT_ENABLED:-true}
      - RATE_LIMIT_STRATEGY=${RATE_LIMIT_STRATEGY:-rolling}
      - RATE_LIMIT_STORAGE_PATH=${RATE_LIMIT_STORAGE_PATH:-/app/data/rate_limits.db}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-0}
      - RATE_LIMIT_PER_HOUR=${RATE_LIMIT_PER_HOUR:-0}
      - RATE_LIMIT_PER_DAY=${RATE_LIMIT_PER_DAY:-0}
      - RATE_LIMIT_PER_WEEK=${RATE_LIMIT_PER_WEEK:-0}
      - RATE_LIMIT_PER_MONTH=${RATE_LIMIT_PER_MONTH:-0}
    ports:
      - "8001:8001"
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    restart: unless-stopped
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Ollama service for local LLM processing
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s


networks:
  ai-network:
    driver: bridge
    name: ai-task-network

volumes:
  ollama_models: